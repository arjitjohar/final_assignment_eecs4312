{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import num2words\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "import spacy\n",
    "import google_play_scraper \n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Prepreocess your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get each csv file in the directory \n",
    "import glob\n",
    "import os\n",
    "path = os.getcwd()\n",
    "import pandas as pd\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "result = glob.glob('*.{}'.format(extension))\n",
    "\n",
    "#remove the MyAppDescriptions.csv file from the list\n",
    "result.remove('MyAppDescriptions.csv')\n",
    "\n",
    "#remove MyAppFeatures.csv file from the list\n",
    "result.remove('MyAppFeatures.csv')\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "## similiar to lab 2 processing of text\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in result:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    \n",
    "\n",
    "    review = df['Review']\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(len(review)):\n",
    "\n",
    "\n",
    "        review[i] = str(review[i])\n",
    "\n",
    "\n",
    "\n",
    "        #remove punctuation\n",
    "        review[i] = review[i].replace('[^\\w\\s]','')\n",
    "\n",
    "        #remove special characters and emojis and other symbols\n",
    "        review[i] = re.sub('[^A-Za-z0-9]+', ' ', review[i])\n",
    "\n",
    "        #if it is a number or a float, convert it to a word\n",
    "        review[i] = re.sub(r'\\d+', lambda x: num2words.num2words(int(x.group(0))), review[i])\n",
    "\n",
    "        #remove extra white spaces \n",
    "        review[i] = review[i].replace('\\s+', ' ')\n",
    "\n",
    "        #turn all words into lower case\n",
    "        review[i] = review[i].lower()\n",
    "\n",
    "        #remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        review[i] = [word for word in review[i].split() if word not in stop_words]\n",
    "\n",
    "        #lemmatize the words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review[i] = [lemmatizer.lemmatize(word) for word in review[i]]\n",
    "\n",
    "        #join the words back together\n",
    "        review[i] = ' '.join(review[i])        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        #write the review to a csv file\n",
    "        \n",
    "        df['Review'][i] = review[i]\n",
    "\n",
    "        df_new = df[['Package name','Reviewer name','Review','Rating']]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    #save to csv file with the same name as the original csv file and add _cleaned to the end of the file name\n",
    "    df_new.to_csv(file[:-4]+'_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bonus of task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bonus for task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Reccomendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bonus for task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
